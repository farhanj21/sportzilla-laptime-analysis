{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\f-jaf\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2025.11.12)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install selenium pandas webdriver-manager beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1ea42",
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndef scrape_racefacer():\n    # 1. Setup the Browser (Chrome)\n    options = webdriver.ChromeOptions()\n    # Enable headless mode for CI environments (GitHub Actions)\n    if os.environ.get('CI') or os.environ.get('GITHUB_ACTIONS'):\n        options.add_argument(\"--headless=new\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--disable-gpu\")\n        options.add_argument(\"--window-size=1920,1080\")\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n\n    try:\n        url = \"https://www.racefacer.com/en/karting-tracks/pakistan/apexautodromepakistan\"\n        print(f\"Opening {url}...\")\n        driver.get(url)\n\n        # Allow initial load\n        time.sleep(5)\n\n        previous_row_count = 0\n        no_change_count = 0\n        max_no_change = 3  # Stop after 3 consecutive failed attempts\n\n        # 2. Loop to load all data\n        while True:\n            try:\n                # Count current loaded rows\n                rows = driver.find_elements(By.CLASS_NAME, \"row\")\n                current_row_count = len(rows)\n                \n                print(f\"Rows loaded so far: {current_row_count}\")\n\n                # Safety Check: If we clicked but the row count didn't change, stop.\n                if current_row_count == previous_row_count and current_row_count > 0:\n                    no_change_count += 1\n                    print(f\"No new data loaded after click (attempt {no_change_count}/{max_no_change}).\")\n                    if no_change_count >= max_no_change:\n                        print(\"Stopping - reached maximum attempts with no new data.\")\n                        break\n                else:\n                    no_change_count = 0  # Reset counter\n                \n                previous_row_count = current_row_count\n\n                # Try multiple possible selectors for the Load More button\n                load_more_btn = None\n                selectors = [\n                    \".load-more-button\",\n                    \"button.load-more\",\n                    \".load-more\",\n                    \"//button[contains(text(), 'Load more')]\",\n                    \"//a[contains(text(), 'Load more')]\"\n                ]\n                \n                for selector in selectors:\n                    try:\n                        if selector.startswith(\"//\"):\n                            # XPath selector\n                            load_more_btn = WebDriverWait(driver, 3).until(\n                                EC.element_to_be_clickable((By.XPATH, selector))\n                            )\n                        else:\n                            # CSS selector\n                            load_more_btn = WebDriverWait(driver, 3).until(\n                                EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n                            )\n                        print(f\"Found button with selector: {selector}\")\n                        break\n                    except:\n                        continue\n                \n                if not load_more_btn:\n                    print(\"No 'Load more' button found with any selector.\")\n                    break\n                \n                # Scroll to button and click\n                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_btn)\n                time.sleep(0.5)\n                driver.execute_script(\"arguments[0].click();\", load_more_btn)\n                print(\"Clicked 'Load more' button\")\n\n                # Wait for new rows to load\n                time.sleep(2)\n                \n            except Exception as e:\n                print(f\"Exception in load loop: {e}\")\n                break\n\n        # 3. Parse the fully loaded HTML BEFORE closing browser\n        print(\"Parsing final data...\")\n        \n        # Get page source while browser is still open\n        page_html = driver.page_source\n        soup = BeautifulSoup(page_html, 'html.parser')\n        data = []\n\n        # --- Extract Podium (1st, 2nd, 3rd) ---\n        podium = soup.find('div', class_='track_podium')\n        if podium:\n            classes = ['first', 'second', 'third']\n            ranks = [1, 2, 3]\n            for cls, rank in zip(classes, ranks):\n                item = podium.find('a', class_=cls)\n                if item:\n                    try:\n                        name = item.find('div', class_='name').get_text(strip=True)\n                        time_val = item.find('div', class_='time').get_text(strip=True)\n                        date = item.find('div', class_='date').get_text(strip=True)\n                        link = item.get('href')\n                        \n                        data.append({\n                            'Position': rank,\n                            'Name': name,\n                            'Date': date,\n                            'Max km/h': '',\n                            'Max G': '',\n                            'Best Time': time_val,\n                            'Profile URL': link\n                        })\n                    except AttributeError:\n                        continue\n\n        # --- Extract Table Rows (4 onwards) ---\n        rows = soup.find_all('div', class_='row')\n        for row in rows:\n            try:\n                pos_div = row.find('div', class_='position')\n                if not pos_div: continue\n                pos = pos_div.get_text(strip=True)\n\n                name_div = row.find('div', class_='name')\n                name = name_div.get_text(strip=True) if name_div else \"\"\n\n                date_div = row.find('div', class_='date')\n                date = date_div.get_text(strip=True) if date_div else \"\"\n\n                mk = row.find('div', class_='max-km-h')\n                max_km = mk.get_text(strip=True) if mk else \"\"\n\n                mg = row.find('div', class_='max-g')\n                max_g = mg.get_text(strip=True) if mg else \"\"\n\n                time_a = row.find('a', class_='time')\n                if time_a:\n                    time_span = time_a.find('span')\n                    best_time = time_span.get_text(strip=True) if time_span else time_a.get_text(strip=True)\n                else:\n                    best_time = \"\"\n\n                name_link = row.find('a', class_='name-date')\n                link = name_link.get('href') if name_link else \"\"\n\n                data.append({\n                    'Position': pos,\n                    'Name': name,\n                    'Date': date,\n                    'Max km/h': max_km,\n                    'Max G': max_g,\n                    'Best Time': best_time,\n                    'Profile URL': link\n                })\n            except AttributeError:\n                continue\n\n        # 4. Save to CSV\n        if data:\n            df = pd.DataFrame(data)\n            df.drop_duplicates(subset=['Position', 'Name', 'Best Time'], inplace=True)\n            \n            filename = 'data_apex.csv'\n            df.to_csv(filename, index=False)\n            print(f\"Success! Scraped {len(df)} rows. Saved to {filename}\")\n            print(df.head(10))\n        else:\n            print(\"No data found.\")\n    \n    except Exception as e:\n        print(f\"Fatal error during scraping: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    finally:\n        # Always close the driver\n        try:\n            driver.quit()\n            print(\"Browser closed successfully\")\n        except:\n            print(\"Browser already closed\")\n\nif __name__ == \"__main__\":\n    scrape_racefacer()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}