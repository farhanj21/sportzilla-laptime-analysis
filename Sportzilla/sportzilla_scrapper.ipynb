{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\f-jaf\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->webdriver-manager) (2025.11.12)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install selenium pandas webdriver-manager beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1ea42",
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndef scrape_racefacer():\n    # 1. Setup the Browser (Chrome)\n    options = webdriver.ChromeOptions()\n    # Enable headless mode for CI environments (GitHub Actions)\n    if os.environ.get('CI') or os.environ.get('GITHUB_ACTIONS'):\n        options.add_argument(\"--headless\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_argument(\"--disable-dev-shm-usage\")\n        options.add_argument(\"--disable-gpu\")\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n\n    url = \"https://www.racefacer.com/en/karting-tracks/pakistan/sportzillaformulakarting\"\n    print(f\"Opening {url}...\")\n    driver.get(url)\n\n    # Allow initial load\n    time.sleep(3)\n\n    previous_row_count = 0\n\n    # 2. Loop to load all data\n    while True:\n        try:\n            # Count current loaded rows\n            rows = driver.find_elements(By.CLASS_NAME, \"row\")\n            current_row_count = len(rows)\n            \n            print(f\"Rows loaded so far: {current_row_count}\")\n\n            # Safety Check: If we clicked but the row count didn't change, stop.\n            if current_row_count == previous_row_count and current_row_count > 0:\n                print(\"No new data loaded after click. Stopping.\")\n                break\n            \n            previous_row_count = current_row_count\n\n            # Find and Click 'Load More'\n            # We wait up to 5 seconds for the button to be clickable\n            load_more_btn = WebDriverWait(driver, 5).until(\n                EC.element_to_be_clickable((By.CSS_SELECTOR, \".load-more-button\"))\n            )\n            \n            # Scroll to button and click (using JS to avoid interception)\n            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_btn)\n            driver.execute_script(\"arguments[0].click();\", load_more_btn)\n\n            # Smart Wait: Wait until the number of rows actually increases\n            try:\n                WebDriverWait(driver, 10).until(\n                    lambda d: len(d.find_elements(By.CLASS_NAME, \"row\")) > current_row_count\n                )\n            except:\n                print(\"Timed out waiting for new rows. Assuming end of list.\")\n                break\n            \n        except Exception as e:\n            # This block runs when the \"Load More\" button is no longer found (hidden by the website)\n            print(\"No more 'Load more' buttons found or end of data reached.\")\n            break\n\n    # 3. Parse the fully loaded HTML\n    print(\"Parsing final data...\")\n    soup = BeautifulSoup(driver.page_source, 'html.parser')\n    data = []\n\n    # --- Extract Podium (1st, 2nd, 3rd) ---\n    podium = soup.find('div', class_='track_podium')\n    if podium:\n        classes = ['first', 'second', 'third']\n        ranks = [1, 2, 3]\n        for cls, rank in zip(classes, ranks):\n            item = podium.find('a', class_=cls)\n            if item:\n                try:\n                    name = item.find('div', class_='name').get_text(strip=True)\n                    time_val = item.find('div', class_='time').get_text(strip=True)\n                    date = item.find('div', class_='date').get_text(strip=True)\n                    link = item.get('href')\n                    \n                    data.append({\n                        'Position': rank,\n                        'Name': name,\n                        'Date': date,\n                        'Max km/h': '', # Podium usually doesn't show these stats\n                        'Max G': '',\n                        'Best Time': time_val,\n                        'Profile URL': link\n                    })\n                except AttributeError:\n                    continue\n\n    # --- Extract Table Rows (4 onwards) ---\n    rows = soup.find_all('div', class_='row')\n    for row in rows:\n        try:\n            pos_div = row.find('div', class_='position')\n            if not pos_div: continue\n            pos = pos_div.get_text(strip=True)\n\n            name_div = row.find('div', class_='name')\n            name = name_div.get_text(strip=True) if name_div else \"\"\n\n            date_div = row.find('div', class_='date')\n            date = date_div.get_text(strip=True) if date_div else \"\"\n\n            mk = row.find('div', class_='max-km-h')\n            max_km = mk.get_text(strip=True) if mk else \"\"\n\n            mg = row.find('div', class_='max-g')\n            max_g = mg.get_text(strip=True) if mg else \"\"\n\n            # Time is often inside an anchor tag\n            time_a = row.find('a', class_='time')\n            if time_a:\n                time_span = time_a.find('span')\n                best_time = time_span.get_text(strip=True) if time_span else time_a.get_text(strip=True)\n            else:\n                best_time = \"\"\n\n            name_link = row.find('a', class_='name-date')\n            link = name_link.get('href') if name_link else \"\"\n\n            data.append({\n                'Position': pos,\n                'Name': name,\n                'Date': date,\n                'Max km/h': max_km,\n                'Max G': max_g,\n                'Best Time': best_time,\n                'Profile URL': link\n            })\n        except AttributeError:\n            continue\n\n    # 4. Save to CSV\n    if data:\n        df = pd.DataFrame(data)\n        # Clean duplicates just in case\n        df.drop_duplicates(subset=['Position', 'Name', 'Best Time'], inplace=True)\n        \n        filename = 'race_results_full.csv'\n        df.to_csv(filename, index=False)\n        print(f\"Success! Scraped {len(df)} rows. Saved to {filename}\")\n        print(df.head())\n    else:\n        print(\"No data found.\")\n\n    driver.quit()\n\nif __name__ == \"__main__\":\n    scrape_racefacer()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}